# -*- coding: utf-8 -*-
"""facerec

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJvnBrYGGeS6tc8OSm8l5FGi2r56kZql
"""

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
from skimage.io import imread
import warnings
warnings.filterwarnings('ignore')

# Mount Google Drive with error handling
from google.colab import drive
import os

def mount_drive_safely():
    """
    Mount Google Drive with proper error handling
    """
    try:
        # Check if already mounted
        if os.path.exists('/content/drive/MyDrive'):
            print("Google Drive already mounted!")
            return True

        # Try mounting with force remount
        drive.mount('/content/drive', force_remount=True)
        print("Google Drive mounted successfully!")
        return True

    except Exception as e:
        print(f"Drive mount failed: {e}")
        print("\nTrying alternative methods...")

        try:
            # Alternative: mount with timeout
            drive.mount('/content/drive', timeout_ms=120000)  # 2 minutes timeout
            return True
        except:
            print("All mount attempts failed!")
            print("\nPlease try these solutions:")
            print("1. Restart runtime: Runtime -> Restart runtime")
            print("2. Clear browser cache and cookies")
            print("3. Use incognito/private browsing mode")
            print("4. Manually run: drive.mount('/content/drive', force_remount=True)")
            return False

# Mount the drive
if not mount_drive_safely():
    # If mounting fails, provide manual instructions
    print("\n" + "="*60)
    print("MANUAL SETUP REQUIRED")
    print("="*60)
    print("1. Run this cell separately:")
    print("   from google.colab import drive")
    print("   drive.mount('/content/drive', force_remount=True)")
    print("2. Follow the authentication steps")
    print("3. Then run the rest of the code")
    print("="*60)

def load_images_from_folder(folder_path, target_size=(92, 112)):
    """
    Load images from ORL dataset folder structure
    """
    images = []
    labels = []

    print("Loading images from dataset...")
    for subdir in sorted(os.listdir(folder_path)):
        subfolder_path = os.path.join(folder_path, subdir)
        if not os.path.isdir(subfolder_path):
            continue

        # Extract label from folder name (s1 -> 1, s2 -> 2, etc.)
        label = int(subdir[1:])

        for filename in sorted(os.listdir(subfolder_path)):
            image_path = os.path.join(subfolder_path, filename)
            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

            if image is not None:
                image = cv2.resize(image, target_size)
                images.append(image)
                labels.append(label)

    return np.array(images), np.array(labels)

def preprocess_data(images):
    """
    Flatten images and normalize pixel values
    """
    nsamples, nx, ny = images.shape
    flattened_images = images.reshape((nsamples, nx * ny))

    # Normalize pixel values to [0, 1]
    flattened_images = flattened_images.astype('float32') / 255.0

    return flattened_images

def apply_pca(X_train, X_test, n_components=80):
    """
    Apply PCA dimensionality reduction
    """
    pca = PCA(n_components=n_components, random_state=42)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    return X_train_pca, X_test_pca, pca

def apply_lda(X_train, X_test, y_train):
    """
    Apply LDA dimensionality reduction
    """
    lda = LDA()
    X_train_lda = lda.fit_transform(X_train, y_train)
    X_test_lda = lda.transform(X_test)

    return X_train_lda, X_test_lda, lda

def train_and_evaluate_models(X_train, X_test, y_train, y_test, method_name):
    """
    Train and evaluate all classifiers
    """
    # Initialize classifiers
    classifiers = {
        'SVM': SVC(kernel='rbf', C=1, gamma='scale', random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),
        'KNN': KNeighborsClassifier(n_neighbors=5)
    }

    results = {}
    trained_models = {}

    print(f"\n{'='*50}")
    print(f"Training models with {method_name}")
    print(f"{'='*50}")

    for name, classifier in classifiers.items():
        print(f"\nTraining {name}...")

        # Train the model
        classifier.fit(X_train, y_train)

        # Make predictions
        y_pred = classifier.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Store results
        results[name] = {
            'accuracy': accuracy,
            'y_pred': y_pred,
            'classifier': classifier
        }

        trained_models[name] = classifier

        print(f"{name} Accuracy: {accuracy:.4f}")

    return results, trained_models

def plot_explained_variance(pca):
    """
    Plot PCA explained variance ratio
    """
    plt.figure(figsize=(12, 5))

    # Individual explained variance
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
             pca.explained_variance_ratio_, 'bo-')
    plt.xlabel('Principal Component')
    plt.ylabel('Explained Variance Ratio')
    plt.title('Individual Explained Variance by PC')
    plt.grid(True)

    # Cumulative explained variance
    plt.subplot(1, 2, 2)
    cumsum = np.cumsum(pca.explained_variance_ratio_)
    plt.plot(range(1, len(cumsum) + 1), cumsum, 'ro-')
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Cumulative Explained Variance')
    plt.grid(True)

    # Add horizontal line at 95% variance
    plt.axhline(y=0.95, color='g', linestyle='--', label='95% Variance')
    plt.legend()

    plt.tight_layout()
    plt.show()

    print(f"Total variance explained by {len(pca.explained_variance_ratio_)} components: {cumsum[-1]:.4f}")

def plot_sample_predictions(X_test_original, y_test, results, method_name, n_samples=15):
    """
    Visualize sample predictions for each classifier
    """
    n_classifiers = len(results)

    plt.figure(figsize=(20, 4 * n_classifiers))

    for idx, (clf_name, result) in enumerate(results.items()):
        y_pred = result['y_pred']

        for i in range(n_samples):
            plt.subplot(n_classifiers, n_samples, idx * n_samples + i + 1)
            plt.imshow(X_test_original[i], cmap='gray')

            # Color coding: green for correct, red for incorrect
            color = 'green' if y_pred[i] == y_test[i] else 'red'
            plt.title(f'Pred: {y_pred[i]}\nTrue: {y_test[i]}', color=color, fontsize=8)
            plt.axis('off')

            # Add classifier name on the first image of each row
            if i == 0:
                plt.ylabel(f'{clf_name}\n({method_name})', rotation=90, fontsize=12)

    plt.suptitle(f'Sample Predictions - {method_name}', fontsize=16)
    plt.tight_layout()
    plt.show()

def plot_confusion_matrices(y_test, results, method_name):
    """
    Plot confusion matrices for all classifiers
    """
    n_classifiers = len(results)
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()

    for idx, (clf_name, result) in enumerate(results.items()):
        y_pred = result['y_pred']
        cm = confusion_matrix(y_test, y_pred)

        sns.heatmap(cm, annot=False, cmap='Blues', ax=axes[idx])
        axes[idx].set_title(f'{clf_name} - {method_name}')
        axes[idx].set_xlabel('Predicted')
        axes[idx].set_ylabel('Actual')

    plt.tight_layout()
    plt.show()

def compare_methods_performance(pca_results, lda_results):
    """
    Compare PCA vs LDA performance across all classifiers
    """
    classifiers = list(pca_results.keys())
    pca_accuracies = [pca_results[clf]['accuracy'] for clf in classifiers]
    lda_accuracies = [lda_results[clf]['accuracy'] for clf in classifiers]

    x = np.arange(len(classifiers))
    width = 0.35

    plt.figure(figsize=(12, 8))

    bars1 = plt.bar(x - width/2, pca_accuracies, width, label='PCA', alpha=0.8, color='skyblue')
    bars2 = plt.bar(x + width/2, lda_accuracies, width, label='LDA', alpha=0.8, color='lightcoral')

    plt.xlabel('Classifiers')
    plt.ylabel('Accuracy')
    plt.title('PCA vs LDA: Classifier Performance Comparison')
    plt.xticks(x, classifiers)
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

    for bar in bars2:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

    plt.ylim(0, 1.1)
    plt.tight_layout()
    plt.show()

    # Print comparison table
    print("\n" + "="*60)
    print("PERFORMANCE COMPARISON TABLE")
    print("="*60)
    print(f"{'Classifier':<15} {'PCA Accuracy':<15} {'LDA Accuracy':<15} {'Difference':<12}")
    print("-"*60)

    for clf in classifiers:
        pca_acc = pca_results[clf]['accuracy']
        lda_acc = lda_results[clf]['accuracy']
        diff = lda_acc - pca_acc
        print(f"{clf:<15} {pca_acc:<15.4f} {lda_acc:<15.4f} {diff:+.4f}")

def test_single_image(image_path, pca_model, lda_model, pca_classifiers, lda_classifiers,
                     target_size=(92, 112)):
    """
    Test a single image with both PCA and LDA models
    """
    # Load and preprocess the image
    image = imread(image_path)
    if len(image.shape) == 3:
        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    image = cv2.resize(image, target_size)
    flattened_image = image.reshape(1, -1).astype('float32') / 255.0

    # Transform using both PCA and LDA
    image_pca = pca_model.transform(flattened_image)
    image_lda = lda_model.transform(flattened_image)

    print(f"\nTesting single image: {image_path}")
    print("="*50)

    # Test with PCA models
    print("PCA Predictions:")
    for clf_name, classifier in pca_classifiers.items():
        pred = classifier.predict(image_pca)[0]
        print(f"  {clf_name}: Subject {pred}")

    print("\nLDA Predictions:")
    for clf_name, classifier in lda_classifiers.items():
        pred = classifier.predict(image_lda)[0]
        print(f"  {clf_name}: Subject {pred}")

    # Visualize the test image
    plt.figure(figsize=(6, 6))
    plt.imshow(image, cmap='gray')
    plt.title(f'Test Image: {os.path.basename(image_path)}')
    plt.axis('off')
    plt.show()

# Main execution
def main():
    # Load dataset
    dataset_folder = "drive/MyDrive/ORL"
    images, labels = load_images_from_folder(dataset_folder)

    print(f"Dataset loaded successfully!")
    print(f"Number of images: {len(images)}")
    print(f"Number of unique subjects: {len(np.unique(labels))}")
    print(f"Image shape: {images[0].shape}")

    # Split the dataset
    X_train, X_test, y_train, y_test = train_test_split(
        images, labels, test_size=0.2, random_state=42, stratify=labels
    )

    print(f"\nDataset split:")
    print(f"Training samples: {len(X_train)}")
    print(f"Testing samples: {len(X_test)}")

    # Preprocess data
    X_train_flat = preprocess_data(X_train)
    X_test_flat = preprocess_data(X_test)

    # Apply PCA
    print("\nApplying PCA...")
    X_train_pca, X_test_pca, pca_model = apply_pca(X_train_flat, X_test_flat, n_components=80)

    # Apply LDA
    print("Applying LDA...")
    X_train_lda, X_test_lda, lda_model = apply_lda(X_train_flat, X_test_flat, y_train)

    print(f"\nDimensionality after PCA: {X_train_pca.shape[1]}")
    print(f"Dimensionality after LDA: {X_train_lda.shape[1]}")

    # Plot PCA explained variance
    plot_explained_variance(pca_model)

    # Train and evaluate models with PCA
    pca_results, pca_classifiers = train_and_evaluate_models(
        X_train_pca, X_test_pca, y_train, y_test, "PCA"
    )

    # Train and evaluate models with LDA
    lda_results, lda_classifiers = train_and_evaluate_models(
        X_train_lda, X_test_lda, y_train, y_test, "LDA"
    )

    # Visualize results
    plot_sample_predictions(X_test, y_test, pca_results, "PCA")
    plot_sample_predictions(X_test, y_test, lda_results, "LDA")

    plot_confusion_matrices(y_test, pca_results, "PCA")
    plot_confusion_matrices(y_test, lda_results, "LDA")

    # Compare methods
    compare_methods_performance(pca_results, lda_results)

    # Test with a single image (example)
    test_image_path = 'drive/MyDrive/ORL/s35/5.pgm'
    if os.path.exists(test_image_path):
        test_single_image(test_image_path, pca_model, lda_model,
                         pca_classifiers, lda_classifiers)

    print("\n" + "="*60)
    print("ANALYSIS COMPLETE!")
    print("="*60)

# Run the main function
if __name__ == "__main__":
    main()